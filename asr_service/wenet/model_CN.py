import torchaudio
import torchaudio.compliance.kaldi as kaldi
from ais_bench.infer.interface import InferSession
import numpy as np
import logging

logger = logging.getLogger("ASR_Service")


class WeNetASRCN:
    def __init__(self, model_path, vocab_path):
        """åˆå§‹åŒ–æ¨¡å‹ï¼ŒåŠ è½½è¯è¡¨"""
        self.vocabulary = load_vocab(vocab_path)
        self.model = InferSession(0, model_path)
        # è·å–æ¨¡å‹è¾“å…¥ç‰¹å¾çš„æœ€å¤§é•¿åº¦
        self.max_len = self.model.get_inputs()[0].shape[1]
        # è®¡ç®—å®‰å…¨çš„éŸ³é¢‘åˆ†æ®µé•¿åº¦ï¼ˆç§’ï¼‰
        # ç•™å‡ºä½™é‡é¿å…è¾¹ç•Œé—®é¢˜ï¼Œä½¿ç”¨80%çš„æœ€å¤§é•¿åº¦
        self.safe_chunk_duration = (self.max_len * 0.01) * 0.8  # ~7.7ç§’

    def transcribe(self, wav_file):
        """æ‰§è¡Œæ¨¡å‹æ¨ç†ï¼Œå°†å½•éŸ³æ–‡ä»¶è½¬ä¸ºæ–‡æœ¬ã€‚æ”¯æŒé•¿éŸ³é¢‘è‡ªåŠ¨åˆ†æ®µã€‚"""
        # åŠ è½½éŸ³é¢‘è·å–æ—¶é•¿
        waveform, sample_rate = torchaudio.load(wav_file)
        audio_duration = waveform.shape[1] / sample_rate

        # å¦‚æœéŸ³é¢‘çŸ­äºå®‰å…¨é•¿åº¦ï¼Œä½¿ç”¨åŸæ–¹æ³•
        if audio_duration <= self.safe_chunk_duration:
            feats_pad, feats_lengths = self.preprocess(wav_file)
            output = self.model.infer([feats_pad, feats_lengths])
            txt = self.post_process(output)
            return txt
        else:
            # ä½¿ç”¨åˆ†æ®µè¯†åˆ«
            logger.info(f"ğŸ“Š éŸ³é¢‘æ—¶é•¿ {audio_duration:.2f}ç§’ï¼Œå¯ç”¨åˆ†æ®µè¯†åˆ«...")
            return self.transcribe_long_audio(wav_file)

    def transcribe_long_audio(self, wav_file):
        """
        é•¿éŸ³é¢‘åˆ†æ®µè¯†åˆ«
        å°†é•¿éŸ³é¢‘åˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µï¼Œåˆ†åˆ«è¯†åˆ«åæ‹¼æ¥
        """
        waveform, sample_rate = torchaudio.load(wav_file)
        waveform, sample_rate = resample(waveform, sample_rate, resample_rate=16000)

        total_samples = waveform.shape[1]
        total_duration = total_samples / sample_rate

        # åˆ†æ®µå‚æ•°
        chunk_duration = self.safe_chunk_duration  # æ¯æ®µæ—¶é•¿ï¼ˆç§’ï¼‰
        overlap_duration = 0.25  # é‡å æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé¿å…æˆªæ–­è¯è¯­
        chunk_samples = int(chunk_duration * sample_rate)
        overlap_samples = int(overlap_duration * sample_rate)
        step_samples = chunk_samples - overlap_samples

        # è®¡ç®—åˆ†æ®µæ•°é‡
        num_chunks = int(np.ceil((total_samples - overlap_samples) / step_samples))
        logger.info(f"ğŸ”ª å°†éŸ³é¢‘åˆ‡åˆ†ä¸º {num_chunks} æ®µè¿›è¡Œè¯†åˆ«...")

        results = []
        for i in range(num_chunks):
            start_sample = i * step_samples
            end_sample = min(start_sample + chunk_samples, total_samples)

            # æå–éŸ³é¢‘ç‰‡æ®µ
            chunk_waveform = waveform[:, start_sample:end_sample]

            # è®¡ç®—è¯¥ç‰‡æ®µçš„ç‰¹å¾
            feature = compute_fbank(chunk_waveform, sample_rate)

            # é¢„å¤„ç†å’Œæ¨ç†
            feats_pad = pad_sequence(feature,
                                    batch_first=True,
                                    padding_value=0,
                                    max_len=self.max_len)
            feats_pad = feats_pad.numpy().astype(np.float32)
            feats_lengths = np.array([feature.shape[0]]).astype(np.int32)

            output = self.model.infer([feats_pad, feats_lengths])
            text = self.post_process(output)

            chunk_start_time = start_sample / sample_rate
            chunk_end_time = end_sample / sample_rate
            logger.info(f"  âœ“ ç‰‡æ®µ {i+1}/{num_chunks} ({chunk_start_time:.1f}s-{chunk_end_time:.1f}s): {text[:30]}...")

            results.append(text)

        # æ‹¼æ¥ç»“æœ
        final_text = self.merge_segments(results)
        logger.info(f"âœ… åˆ†æ®µè¯†åˆ«å®Œæˆï¼Œæ€»æ–‡æœ¬é•¿åº¦: {len(final_text)} å­—ç¬¦")

        return final_text

    def merge_segments(self, segments):
        """
        æ™ºèƒ½æ‹¼æ¥åˆ†æ®µè¯†åˆ«ç»“æœ
        ç”±äºä¸­æ–‡æ²¡æœ‰ç©ºæ ¼ï¼Œç®€å•è¿æ¥å³å¯
        """
        return ''.join(segments)

    def preprocess(self, wav_file):
        """æ•°æ®é¢„å¤„ç†"""
        waveform, sample_rate = torchaudio.load(wav_file)
        # éŸ³é¢‘é‡é‡‡æ ·ï¼Œé‡‡æ ·ç‡16000
        waveform, sample_rate = resample(waveform, sample_rate, resample_rate=16000)
        # è®¡ç®—fbankç‰¹å¾
        feature = compute_fbank(waveform, sample_rate)
        feats_lengths = np.array([feature.shape[0]]).astype(np.int32)

        # æ£€æŸ¥éŸ³é¢‘é•¿åº¦å¹¶æ‰“å°è­¦å‘Š
        feat_len = feature.shape[0]
        max_duration_sec = self.max_len * 0.01  # æ¯å¸§10ms
        actual_duration_sec = feat_len * 0.01
        if feat_len > self.max_len:
            import logging
            logger = logging.getLogger("ASR_Service")
            logger.warning(f"âš ï¸ éŸ³é¢‘æ—¶é•¿({actual_duration_sec:.2f}ç§’)è¶…è¿‡æ¨¡å‹æœ€å¤§é™åˆ¶({max_duration_sec:.2f}ç§’)ï¼Œå°†è¢«æˆªæ–­!")
            logger.warning(f"   å»ºè®®ä½¿ç”¨æ—¶é•¿ â‰¤ {max_duration_sec:.1f}ç§’ çš„éŸ³é¢‘ï¼Œæˆ–ç­‰å¾…åˆ†æ®µè¯†åˆ«åŠŸèƒ½")

        # å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œpaddingï¼Œä½¿ç¬¦åˆæ¨¡å‹è¾“å…¥å°ºå¯¸
        feats_pad = pad_sequence(feature,
                                 batch_first=True,
                                 padding_value=0,
                                 max_len=self.max_len)
        feats_pad = feats_pad.numpy().astype(np.float32)
        return feats_pad, feats_lengths

    def post_process(self, output):
        """å¯¹æ¨¡å‹æ¨ç†ç»“æœè¿›è¡Œåå¤„ç†ï¼Œæ ¹æ®è´ªå¿ƒç­–ç•¥é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„tokenï¼Œå»é™¤é‡å¤å­—ç¬¦å’Œç©ºç™½å­—ç¬¦ï¼Œå¾—åˆ°æœ€ç»ˆæ–‡æœ¬ã€‚"""
        encoder_out_lens, probs_idx = output[1], output[4]
        token_idx_list = probs_idx[0, :, 0][:encoder_out_lens[0]]
        token_idx_list = remove_duplicates_and_blank(token_idx_list)
        text = ''.join(self.vocabulary[token_idx_list])
        return text


def remove_duplicates_and_blank(token_idx_list):
    """å»é™¤é‡å¤å­—ç¬¦å’Œç©ºç™½å­—ç¬¦"""
    res = []
    cur = 0
    BLANK_ID = 0
    while cur < len(token_idx_list):
        if token_idx_list[cur] != BLANK_ID:
            res.append(token_idx_list[cur])
        prev = cur
        while cur < len(token_idx_list) and token_idx_list[cur] == token_idx_list[prev]:
            cur += 1
    return res


def pad_sequence(seq_feature, batch_first=True, padding_value=0, max_len=966):
    """å¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œpaddingï¼Œä½¿ç¬¦åˆæ¨¡å‹è¾“å…¥å°ºå¯¸"""
    feature_shape = seq_feature.shape
    feat_len = feature_shape[0]
    if feat_len > max_len:
        # å¦‚æœè¾“å…¥ç‰¹å¾é•¿åº¦å¤§äºæ¨¡å‹è¾“å…¥å°ºå¯¸ï¼Œåˆ™æˆªæ–­
        seq_feature = seq_feature[:max_len].unsqueeze(0)
        return seq_feature

    batch_size = 1
    trailing_dims = feature_shape[1:]
    if batch_first:
        out_dims = (batch_size, max_len) + trailing_dims
    else:
        out_dims = (max_len, batch_size) + trailing_dims

    out_tensor = seq_feature.data.new(*out_dims).fill_(padding_value)
    if batch_first:
        out_tensor[0, :feat_len, ...] = seq_feature
    else:
        out_tensor[:feat_len, 0, ...] = seq_feature
    return out_tensor


def resample(waveform, sample_rate, resample_rate=16000):
    """éŸ³é¢‘é‡é‡‡æ ·"""
    waveform = torchaudio.transforms.Resample(
        orig_freq=sample_rate, new_freq=resample_rate)(waveform)
    return waveform, resample_rate


def compute_fbank(waveform,
                  sample_rate,
                  num_mel_bins=80,
                  frame_length=25,
                  frame_shift=10,
                  dither=0.0):
    """æå–filter bankéŸ³é¢‘ç‰¹å¾"""
    AMPLIFY_FACTOR = 1 << 15
    waveform = waveform * AMPLIFY_FACTOR
    mat = kaldi.fbank(waveform,
                      num_mel_bins=num_mel_bins,
                      frame_length=frame_length,
                      frame_shift=frame_shift,
                      dither=dither,
                      energy_floor=0.0,
                      sample_frequency=sample_rate)
    return mat


def load_vocab(txt_path):
    """åŠ è½½è¯è¡¨"""
    vocabulary = []
    LEN_OF_VALID_FORMAT = 2
    with open(txt_path, 'r') as fin:
        for line in fin:
            arr = line.strip().split()
            # è¯è¡¨æ ¼å¼ï¼štoken id
            if len(arr) != LEN_OF_VALID_FORMAT:
                raise ValueError(f"Invalid line: {line}. Expect format: token id")
            vocabulary.append(arr[0])
    return np.array(vocabulary)
