"""
LLMå¤§æ¨¡å‹æœåŠ¡ - FastAPIç‰ˆæœ¬
æ”¯æŒAPIå’Œæœ¬åœ°æ¨¡å‹ä¸¤ç§æ¨¡å¼,æä¾›æµå¼å’Œéæµå¼å¯¹è¯
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import sys
import logging
import json
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥é…ç½®
sys.path.insert(0, str(Path(__file__).parent.parent))
from config_loader import get_config

import requests

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("LLM_Service")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="LLMå¤§æ¨¡å‹æœåŠ¡",
    description="æ”¯æŒDeepSeek APIå’Œæœ¬åœ°æ¨¡å‹çš„å¯¹è¯æœåŠ¡",
    version="1.0.0"
)

# è¯·æ±‚ä½“æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str


class ChatRequest(BaseModel):
    message: str
    history: Optional[List[List[str]]] = []
    stream: Optional[bool] = True
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None


class ChatResponse(BaseModel):
    success: bool
    message: str
    model: str


# ==================== DeepSeek API æ¨¡å¼ ====================
def build_messages_from_history(history: List[List[str]], user_msg: str) -> List[Dict[str, str]]:
    """ä»å†å²è®°å½•æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
    llm_config = get_config('llm')
    system_prompt = llm_config.get('api', {}).get('system_prompt', 'You are a helpful assistant.')

    messages = [{"role": "system", "content": system_prompt}]

    for user, ai in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": ai})

    messages.append({"role": "user", "content": user_msg})
    return messages


async def chat_with_deepseek_api_stream(message: str, history: List[List[str]],
                                       max_tokens: Optional[int] = None,
                                       temperature: Optional[float] = None):
    """ä½¿ç”¨DeepSeek APIè¿›è¡Œæµå¼å¯¹è¯"""
    llm_config = get_config('llm')
    api_config = llm_config.get('api', {})

    api_key = api_config.get('api_key')
    api_url = api_config.get('api_url')
    model = api_config.get('model', 'deepseek-chat')

    if not api_key or not api_url:
        raise HTTPException(status_code=500, detail="DeepSeek APIé…ç½®ä¸å®Œæ•´")

    # æ„å»ºè¯·æ±‚
    payload = {
        "model": model,
        "messages": build_messages_from_history(history, message),
        "stream": True,
        "max_tokens": max_tokens or api_config.get('max_tokens', 512),
        "temperature": temperature or api_config.get('temperature', 1.0),
        "top_p": api_config.get('top_p', 0.9)
    }

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    try:
        with requests.post(api_url, headers=headers, json=payload, stream=True, timeout=60) as resp:
            resp.raise_for_status()

            for line in resp.iter_lines():
                if line:
                    try:
                        if line.startswith(b'data: '):
                            line = line[6:]
                        chunk = line.decode("utf-8").strip()

                        if chunk == "[DONE]":
                            break

                        data = json.loads(chunk)
                        delta = data.get("choices", [{}])[0].get("delta", {}).get("content", "")

                        if delta:
                            # è¿”å›SSEæ ¼å¼
                            yield f"data: {json.dumps({'delta': delta})}\n\n"

                    except json.JSONDecodeError:
                        continue
                    except Exception as e:
                        logger.error(f"å¤„ç†æµå¼å“åº”å‡ºé”™: {e}")
                        continue

            # å‘é€ç»“æŸæ ‡è®°
            yield f"data: {json.dumps({'done': True})}\n\n"

    except requests.exceptions.RequestException as e:
        logger.error(f"DeepSeek APIè¯·æ±‚å¤±è´¥: {e}")
        yield f"data: {json.dumps({'error': str(e)})}\n\n"


async def chat_with_deepseek_api(message: str, history: List[List[str]],
                                 max_tokens: Optional[int] = None,
                                 temperature: Optional[float] = None) -> str:
    """ä½¿ç”¨DeepSeek APIè¿›è¡Œéæµå¼å¯¹è¯"""
    llm_config = get_config('llm')
    api_config = llm_config.get('api', {})

    api_key = api_config.get('api_key')
    api_url = api_config.get('api_url')
    model = api_config.get('model', 'deepseek-chat')

    payload = {
        "model": model,
        "messages": build_messages_from_history(history, message),
        "stream": False,
        "max_tokens": max_tokens or api_config.get('max_tokens', 512),
        "temperature": temperature or api_config.get('temperature', 1.0),
        "top_p": api_config.get('top_p', 0.9)
    }

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    try:
        resp = requests.post(api_url, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        result = resp.json()
        return result['choices'][0]['message']['content']
    except Exception as e:
        logger.error(f"DeepSeek APIè¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"APIè°ƒç”¨å¤±è´¥: {str(e)}")


# ==================== æœ¬åœ°æ¨¡å‹æ¨¡å¼ (é¢„ç•™) ====================
# æœ¬åœ°æ¨¡å‹çš„å®ç°å¯ä»¥æ ¹æ®éœ€è¦æ·»åŠ 
local_model = None
local_tokenizer = None


def init_local_model():
    """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
    # è¿™é‡Œå¯ä»¥æ·»åŠ æœ¬åœ°æ¨¡å‹åŠ è½½é€»è¾‘
    # ä¾‹å¦‚åŠ è½½ Qwen1.5-0.5b æˆ– TinyLlama
    pass


# ==================== FastAPI è·¯ç”± ====================
@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ LLMæœåŠ¡æ­£åœ¨å¯åŠ¨...")

    llm_config = get_config('llm')
    mode = llm_config.get('mode', 'api')

    if mode == 'local':
        logger.info("ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ¨¡å¼")
        init_local_model()
    else:
        logger.info("ä½¿ç”¨APIæ¨¡å¼")

    logger.info("âœ… LLMæœåŠ¡å¯åŠ¨å®Œæˆ!")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    llm_config = get_config('llm')
    return {
        "service": "LLMå¤§æ¨¡å‹æœåŠ¡",
        "status": "running",
        "mode": llm_config.get('mode'),
        "model": llm_config.get('api', {}).get('model') if llm_config.get('mode') == 'api' else llm_config.get('local', {}).get('model_name')
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "llm",
        "mode": get_config('llm.mode')
    }


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼å¯¹è¯æ¥å£
    è¿”å›Server-Sent Events (SSE)æµ
    """
    llm_config = get_config('llm')
    mode = llm_config.get('mode', 'api')

    logger.info(f"æ”¶åˆ°æµå¼å¯¹è¯è¯·æ±‚: {request.message[:50]}...")

    if mode == 'api':
        return StreamingResponse(
            chat_with_deepseek_api_stream(
                request.message,
                request.history,
                request.max_tokens,
                request.temperature
            ),
            media_type="text/event-stream"
        )
    else:
        # æœ¬åœ°æ¨¡å‹æµå¼è¾“å‡º(é¢„ç•™)
        raise HTTPException(status_code=501, detail="æœ¬åœ°æ¨¡å‹æµå¼è¾“å‡ºæš‚æœªå®ç°")


@app.post("/chat")
async def chat(request: ChatRequest):
    """
    éæµå¼å¯¹è¯æ¥å£
    è¿”å›å®Œæ•´å“åº”
    """
    llm_config = get_config('llm')
    mode = llm_config.get('mode', 'api')

    logger.info(f"æ”¶åˆ°å¯¹è¯è¯·æ±‚: {request.message[:50]}...")

    try:
        if mode == 'api':
            response_text = await chat_with_deepseek_api(
                request.message,
                request.history,
                request.max_tokens,
                request.temperature
            )
            return ChatResponse(
                success=True,
                message=response_text,
                model=llm_config.get('api', {}).get('model', 'deepseek-chat')
            )
        else:
            # æœ¬åœ°æ¨¡å‹æ¨ç†(é¢„ç•™)
            raise HTTPException(status_code=501, detail="æœ¬åœ°æ¨¡å‹æ¨ç†æš‚æœªå®ç°")

    except Exception as e:
        logger.error(f"å¯¹è¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/reload_config")
async def reload_config():
    """é‡æ–°åŠ è½½é…ç½®"""
    try:
        from config_loader import reload_config
        reload_config()
        return {"success": True, "message": "é…ç½®é‡æ–°åŠ è½½æˆåŠŸ"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"é…ç½®é‡æ–°åŠ è½½å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    # ä»é…ç½®æ–‡ä»¶è¯»å–ç«¯å£
    port = get_config('services.llm', 5002)

    logger.info(f"LLMæœåŠ¡å¯åŠ¨åœ¨ç«¯å£: {port}")

    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
