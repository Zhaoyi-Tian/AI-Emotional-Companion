"""
LLMå¤§æ¨¡å‹æœåŠ¡ - FastAPIç‰ˆæœ¬
æ”¯æŒAPIå’Œæœ¬åœ°æ¨¡å‹ä¸¤ç§æ¨¡å¼,æä¾›æµå¼å’Œéæµå¼å¯¹è¯
"""

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import uvicorn
import sys
import logging
import json
from pathlib import Path

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥é…ç½®
sys.path.insert(0, str(Path(__file__).parent.parent))
from config_loader import get_config

import requests

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("LLM_Service")

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="LLMå¤§æ¨¡å‹æœåŠ¡",
    description="æ”¯æŒDeepSeek APIå’Œæœ¬åœ°æ¨¡å‹çš„å¯¹è¯æœåŠ¡",
    version="1.0.0"
)

# è¯·æ±‚ä½“æ¨¡å‹
class ChatMessage(BaseModel):
    role: str
    content: str


class ChatRequest(BaseModel):
    message: str
    history: Optional[List[List[str]]] = []
    stream: Optional[bool] = True
    max_tokens: Optional[int] = None
    temperature: Optional[float] = None


class ChatResponse(BaseModel):
    success: bool
    message: str
    model: str


# ==================== DeepSeek API æ¨¡å¼ ====================
def build_messages_from_history(history: List[List[str]], user_msg: str) -> List[Dict[str, str]]:
    """ä»å†å²è®°å½•æ„å»ºæ¶ˆæ¯åˆ—è¡¨"""
    llm_config = get_config('llm')
    system_prompt = llm_config.get('api', {}).get('system_prompt', 'You are a helpful assistant.')

    messages = [{"role": "system", "content": system_prompt}]

    for user, ai in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": ai})

    messages.append({"role": "user", "content": user_msg})
    return messages


async def chat_with_deepseek_api_stream(message: str, history: List[List[str]],
                                       max_tokens: Optional[int] = None,
                                       temperature: Optional[float] = None):
    """ä½¿ç”¨DeepSeek APIè¿›è¡Œæµå¼å¯¹è¯"""
    llm_config = get_config('llm')
    api_config = llm_config.get('api', {})

    api_key = api_config.get('api_key')
    api_url = api_config.get('api_url')
    model = api_config.get('model', 'deepseek-chat')

    if not api_key or not api_url:
        raise HTTPException(status_code=500, detail="DeepSeek APIé…ç½®ä¸å®Œæ•´")

    # æ„å»ºè¯·æ±‚
    payload = {
        "model": model,
        "messages": build_messages_from_history(history, message),
        "stream": True,
        "max_tokens": max_tokens or api_config.get('max_tokens', 512),
        "temperature": temperature or api_config.get('temperature', 1.0),
        "top_p": api_config.get('top_p', 0.9)
    }

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    try:
        with requests.post(api_url, headers=headers, json=payload, stream=True, timeout=60) as resp:
            resp.raise_for_status()

            for line in resp.iter_lines():
                if line:
                    try:
                        if line.startswith(b'data: '):
                            line = line[6:]
                        chunk = line.decode("utf-8").strip()

                        if chunk == "[DONE]":
                            break

                        data = json.loads(chunk)
                        delta = data.get("choices", [{}])[0].get("delta", {}).get("content", "")

                        if delta:
                            # è¿”å›SSEæ ¼å¼
                            yield f"data: {json.dumps({'delta': delta})}\n\n"

                    except json.JSONDecodeError:
                        continue
                    except Exception as e:
                        logger.error(f"å¤„ç†æµå¼å“åº”å‡ºé”™: {e}")
                        continue

            # å‘é€ç»“æŸæ ‡è®°
            yield f"data: {json.dumps({'done': True})}\n\n"

    except requests.exceptions.RequestException as e:
        logger.error(f"DeepSeek APIè¯·æ±‚å¤±è´¥: {e}")
        yield f"data: {json.dumps({'error': str(e)})}\n\n"


async def chat_with_deepseek_api(message: str, history: List[List[str]],
                                 max_tokens: Optional[int] = None,
                                 temperature: Optional[float] = None) -> str:
    """ä½¿ç”¨DeepSeek APIè¿›è¡Œéæµå¼å¯¹è¯"""
    llm_config = get_config('llm')
    api_config = llm_config.get('api', {})

    api_key = api_config.get('api_key')
    api_url = api_config.get('api_url')
    model = api_config.get('model', 'deepseek-chat')

    payload = {
        "model": model,
        "messages": build_messages_from_history(history, message),
        "stream": False,
        "max_tokens": max_tokens or api_config.get('max_tokens', 512),
        "temperature": temperature or api_config.get('temperature', 1.0),
        "top_p": api_config.get('top_p', 0.9)
    }

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    try:
        resp = requests.post(api_url, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        result = resp.json()
        return result['choices'][0]['message']['content']
    except Exception as e:
        logger.error(f"DeepSeek APIè¯·æ±‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"APIè°ƒç”¨å¤±è´¥: {str(e)}")


# ==================== æœ¬åœ°æ¨¡å‹æ¨¡å¼ ====================
local_model = None
local_tokenizer = None
local_model_type = None


def set_mindspore_env():
    """è®¾ç½®MindSporeç¯å¢ƒå˜é‡,é˜²æ­¢æ¨¡å‹åŠ è½½å´©æºƒ"""
    import os
    os.environ['TE_PARALLEL_COMPILER'] = '1'
    os.environ['MAX_COMPILE_CORE_NUMBER'] = '1'
    os.environ['MS_BUILD_PROCESS_NUM'] = '1'
    os.environ['MAX_RUNTIME_CORE_NUMBER'] = '1'
    os.environ['MS_ENABLE_IO_REUSE'] = '1'
    logger.info("âœ… MindSporeç¯å¢ƒå˜é‡å·²è®¾ç½®")


def init_local_model():
    """åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹"""
    global local_model, local_tokenizer, local_model_type

    llm_config = get_config('llm')
    local_config = llm_config.get('local', {})
    model_name = local_config.get('model_name', 'qwen')

    logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}")

    # è®¾ç½®ç¯å¢ƒå˜é‡
    set_mindspore_env()

    try:
        import mindspore
        from mindnlp.transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
        from mindspore._c_expression import disable_multi_thread
        disable_multi_thread()

        if model_name.lower() in ['qwen', 'qwen1.5-0.5b']:
            # åŠ è½½Qwenæ¨¡å‹
            model_path = local_config.get('qwen_model_path', '/home/HwHiAiUser/.mindnlp/model/Qwen/Qwen1.5-0.5B-Chat')
            logger.info(f"åŠ è½½Qwenæ¨¡å‹: {model_path}")

            local_tokenizer = AutoTokenizer.from_pretrained(model_path, ms_dtype=mindspore.float16)
            local_model = AutoModelForCausalLM.from_pretrained(model_path, ms_dtype=mindspore.float16)
            local_model_type = 'qwen'
            logger.info("âœ… Qwen1.5-0.5Bæ¨¡å‹åŠ è½½æˆåŠŸ")

        elif model_name.lower() in ['tinyllama', 'tiny']:
            # åŠ è½½TinyLlamaæ¨¡å‹
            model_path = local_config.get('tinyllama_model_path', 'TinyLlama/TinyLlama-1.1B-Chat-v1.0')
            logger.info(f"åŠ è½½TinyLlamaæ¨¡å‹: {model_path}")

            local_tokenizer = AutoTokenizer.from_pretrained(model_path)
            local_model = AutoModelForCausalLM.from_pretrained(model_path, ms_dtype=mindspore.float16)
            local_model_type = 'tinyllama'
            logger.info("âœ… TinyLlamaæ¨¡å‹åŠ è½½æˆåŠŸ")

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model_name}")

    except ImportError as e:
        logger.error(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        logger.error("è¯·ç¡®ä¿åœ¨llmç¯å¢ƒä¸­å®‰è£…äº†mindsporeå’Œmindnlp")
        raise
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        raise


async def chat_with_local_model_stream(message: str, history: List[List[str]],
                                       max_tokens: Optional[int] = None,
                                       temperature: Optional[float] = None):
    """ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œæµå¼å¯¹è¯"""
    global local_model, local_tokenizer, local_model_type

    if local_model is None or local_tokenizer is None:
        raise HTTPException(status_code=503, detail="æœ¬åœ°æ¨¡å‹æœªåŠ è½½")

    from mindnlp.transformers import TextIteratorStreamer
    from threading import Thread
    import mindspore

    llm_config = get_config('llm')
    local_config = llm_config.get('local', {})
    system_prompt = local_config.get('system_prompt', 'You are a helpful and friendly chatbot')

    max_new_tokens = max_tokens or local_config.get('max_tokens', 128)
    temp = temperature or local_config.get('temperature', 1.0)

    try:
        if local_model_type == 'qwen':
            # Qwenæ¨¡å‹çš„è¾“å…¥æ ¼å¼
            messages = [{'role': 'system', 'content': system_prompt}]
            for user_msg, ai_msg in history:
                messages.append({'role': 'user', 'content': user_msg})
                messages.append({'role': 'assistant', 'content': ai_msg})
            messages.append({'role': 'user', 'content': message})

            input_ids = local_tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                return_tensors="ms",
                tokenize=True
            )

        else:  # tinyllama
            # TinyLlamaçš„è¾“å…¥æ ¼å¼
            history_format = history + [[message, ""]]
            messages = "</s>".join(["</s>".join(["\n<|user|>:" + item[0], "\n<|assistant|>:" + item[1]])
                                   for item in history_format])
            model_inputs = local_tokenizer([messages], return_tensors="ms")
            input_ids = model_inputs['input_ids']

        # åˆ›å»ºæµå¼è¾“å‡º
        streamer = TextIteratorStreamer(local_tokenizer, timeout=300, skip_prompt=True, skip_special_tokens=True)

        generate_kwargs = dict(
            input_ids=input_ids,
            streamer=streamer,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            top_p=0.9,
            temperature=temp,
            num_beams=1,
            use_cache=True
        )

        # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
        thread = Thread(target=local_model.generate, kwargs=generate_kwargs)
        thread.start()

        # æµå¼è¾“å‡º
        partial_message = ""
        for new_token in streamer:
            if '</s>' in new_token:
                break
            partial_message += new_token
            yield f"data: {json.dumps({'delta': new_token})}\n\n"

        yield f"data: {json.dumps({'done': True})}\n\n"

    except Exception as e:
        logger.error(f"æœ¬åœ°æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
        yield f"data: {json.dumps({'error': str(e)})}\n\n"


async def chat_with_local_model(message: str, history: List[List[str]],
                                max_tokens: Optional[int] = None,
                                temperature: Optional[float] = None) -> str:
    """ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œéæµå¼å¯¹è¯"""
    full_response = ""

    async for chunk in chat_with_local_model_stream(message, history, max_tokens, temperature):
        if chunk.startswith('data: '):
            data = json.loads(chunk[6:])
            if data.get('delta'):
                full_response += data['delta']
            elif data.get('error'):
                raise HTTPException(status_code=500, detail=data['error'])

    return full_response


# ==================== FastAPI è·¯ç”± ====================
@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ LLMæœåŠ¡æ­£åœ¨å¯åŠ¨...")

    llm_config = get_config('llm')
    mode = llm_config.get('mode', 'api')

    if mode == 'local':
        logger.info("ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ¨¡å¼")
        init_local_model()
    else:
        logger.info("ä½¿ç”¨APIæ¨¡å¼")

    logger.info("âœ… LLMæœåŠ¡å¯åŠ¨å®Œæˆ!")


@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    llm_config = get_config('llm')
    return {
        "service": "LLMå¤§æ¨¡å‹æœåŠ¡",
        "status": "running",
        "mode": llm_config.get('mode'),
        "model": llm_config.get('api', {}).get('model') if llm_config.get('mode') == 'api' else llm_config.get('local', {}).get('model_name')
    }


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "service": "llm",
        "mode": get_config('llm.mode')
    }


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """
    æµå¼å¯¹è¯æ¥å£
    è¿”å›Server-Sent Events (SSE)æµ
    """
    llm_config = get_config('llm')
    mode = llm_config.get('mode', 'api')

    logger.info(f"æ”¶åˆ°æµå¼å¯¹è¯è¯·æ±‚: {request.message[:50]}...")

    if mode == 'api':
        return StreamingResponse(
            chat_with_deepseek_api_stream(
                request.message,
                request.history,
                request.max_tokens,
                request.temperature
            ),
            media_type="text/event-stream"
        )
    else:  # local
        return StreamingResponse(
            chat_with_local_model_stream(
                request.message,
                request.history,
                request.max_tokens,
                request.temperature
            ),
            media_type="text/event-stream"
        )


@app.post("/chat")
async def chat(request: ChatRequest):
    """
    éæµå¼å¯¹è¯æ¥å£
    è¿”å›å®Œæ•´å“åº”
    """
    llm_config = get_config('llm')
    mode = llm_config.get('mode', 'api')

    logger.info(f"æ”¶åˆ°å¯¹è¯è¯·æ±‚: {request.message[:50]}...")

    try:
        if mode == 'api':
            response_text = await chat_with_deepseek_api(
                request.message,
                request.history,
                request.max_tokens,
                request.temperature
            )
            model_name = llm_config.get('api', {}).get('model', 'deepseek-chat')
        else:  # local
            response_text = await chat_with_local_model(
                request.message,
                request.history,
                request.max_tokens,
                request.temperature
            )
            model_name = llm_config.get('local', {}).get('model_name', 'local-model')

        return ChatResponse(
            success=True,
            message=response_text,
            model=model_name
        )

    except Exception as e:
        logger.error(f"å¯¹è¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/reload_config")
async def reload_config_endpoint():
    """é‡æ–°åŠ è½½é…ç½®å¹¶é‡æ–°åˆå§‹åŒ–æ¨¡å‹"""
    global local_model, local_tokenizer, local_model_type

    try:
        from config_loader import reload_config as reload_config_file
        reload_config_file()

        llm_config = get_config('llm')
        mode = llm_config.get('mode', 'api')

        logger.info(f"é…ç½®å·²é‡æ–°åŠ è½½,å½“å‰æ¨¡å¼: {mode}")

        # å¦‚æœæ˜¯æœ¬åœ°æ¨¡å¼,é‡æ–°åŠ è½½æ¨¡å‹
        if mode == 'local':
            logger.info("æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å¼,æ­£åœ¨é‡æ–°åŠ è½½æ¨¡å‹...")

            # æ¸…ç†æ—§æ¨¡å‹(é‡Šæ”¾å†…å­˜)
            if local_model is not None:
                logger.info("æ¸…ç†æ—§æ¨¡å‹...")
                local_model = None
                local_tokenizer = None
                local_model_type = None

                # å¼ºåˆ¶åƒåœ¾å›æ”¶
                import gc
                gc.collect()

            # é‡æ–°åŠ è½½æ¨¡å‹
            init_local_model()

            model_name = llm_config.get('local', {}).get('model_name')
            return {
                "success": True,
                "message": f"é…ç½®é‡æ–°åŠ è½½æˆåŠŸ,æœ¬åœ°æ¨¡å‹ {model_name} å·²åŠ è½½",
                "mode": "local",
                "model": model_name
            }
        else:
            # APIæ¨¡å¼ä¸éœ€è¦åŠ è½½æ¨¡å‹
            return {
                "success": True,
                "message": "é…ç½®é‡æ–°åŠ è½½æˆåŠŸ,ä½¿ç”¨APIæ¨¡å¼",
                "mode": "api",
                "model": llm_config.get('api', {}).get('model')
            }

    except Exception as e:
        logger.error(f"é…ç½®é‡æ–°åŠ è½½å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"é…ç½®é‡æ–°åŠ è½½å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    # ä»é…ç½®æ–‡ä»¶è¯»å–ç«¯å£
    port = get_config('services.llm', 5002)

    logger.info(f"LLMæœåŠ¡å¯åŠ¨åœ¨ç«¯å£: {port}")

    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
